from seleniumbase import SB
from bs4 import BeautifulSoup
import re
import time
import os
import json

BASE_URL = "https://greektube.pro"
START_URLS = [
    "https://greektube.pro/movies?order=created_at%3Adesc",
    "https://greektube.pro/movies?order=created_at%3Adesc&page=2"
]
OUTPUT_FILE = "GrTube.m3u"

# --- Î’ÎŸÎ—Î˜Î—Î¤Î™ÎšÎ•Î£ Î£Î¥ÎÎ‘Î¡Î¤Î—Î£Î•Î™Î£ ---

def extract_bootstrap_url(soup):
    """
    Î¨Î¬Ï‡Î½ÎµÎ¹ Î¼Î­ÏƒÎ± ÏƒÏ„Î± <script> Î³Î¹Î± Ï„Î¿ window.bootstrapData ÎºÎ±Î¹ ÎµÎ¾Î¬Î³ÎµÎ¹ Ï„Î¿ 'src' Ï„Î¿Ï… Î²Î¯Î½Ï„ÎµÎ¿.
    Î‘Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿ Î³Î¹Î± Ï„Î±Î¹Î½Î¯ÎµÏ‚ Ï€Î¿Ï… ÎºÏÏÎ²Î¿Ï…Î½ Ï„Î¿Î½ player ÏƒÎµ JS objects.
    """
    try:
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'window.bootstrapData' in script.string:
                # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Regex Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ src Î¼Î­ÏƒÎ± ÏƒÏ„Î¿ video object
                # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ: 'video': { ... 'src': 'URL'
                # Î¤Î¿ pattern Ï€Î¹Î¬Î½ÎµÎ¹ Ï„Î¿ src Ï€Î¿Ï… ÎµÎ¯Î½Î±Î¹ Î¼Î­ÏƒÎ± ÏƒÎµ single quotes
                match = re.search(r"'video':\s*\{[^}]*?'src':\s*'([^']+)'", script.string, re.DOTALL)
                
                # Î‘Î½ Î´ÎµÎ½ Ï€ÎµÏ„ÏÏ‡ÎµÎ¹ Ï„Î¿ Î±ÎºÏÎ¹Î²Î­Ï‚, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Ï€Î¹Î¿ Î³ÎµÎ½Î¹ÎºÎ¬ Î³Î¹Î± src Ï€Î¿Ï… Î¼Î¿Î¹Î¬Î¶ÎµÎ¹ Î¼Îµ embed
                if not match:
                    match = re.search(r"'src':\s*'([^']+(?:upns\.pro|embed|player)[^']*)'", script.string)
                
                if match:
                    url = match.group(1)
                    # ÎšÎ±Î¸Î±ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î± escaped slashes
                    url = url.replace(r'\/', '/')
                    return url
    except Exception as e:
        print(f"Error parsing bootstrap data: {e}")
    return None

def extract_links_from_source(source):
    video_url = None
    sub_url = None
    clean_source = source.replace(r'\/', '/')
    
    # Regex Î³Î¹Î± Î²Î¯Î½Ï„ÎµÎ¿ ÎºÎ±Î¹ master files (.txt)
    vid_regex = r'(https?://[^"\'<>]+\.(?:mp4|m3u8|txt)(?:[^"\'<>]*)?)'
    sub_regex = r'(https?://[^"\'<>]+\.(?:vtt|srt)(?:[^"\'<>]*)?)'
    
    vid_matches = re.findall(vid_regex, clean_source)
    for match in vid_matches:
        if "google" not in match and "facebook" not in match and "w3.org" not in match:
            video_url = match
            break
            
    sub_match = re.search(sub_regex, clean_source)
    if sub_match: sub_url = sub_match.group(1)
    return video_url, sub_url

def get_stream_and_sub(sb, watch_url):
    video_url = None
    sub_url = None
    final_referer = watch_url 
    
    try:
        # 1. Î‘Î½Î¿Î¯Î³Î¿Ï…Î¼Îµ Ï„Î·Î½ Î±ÏÏ‡Î¹ÎºÎ® ÏƒÎµÎ»Î¯Î´Î± watch
        sb.uc_open_with_reconnect(watch_url, reconnect_time=3)
        sb.sleep(2) 
        
        source = sb.get_page_source()
        soup = BeautifulSoup(source, 'html.parser')

        # --- Î•Î›Î•Î“Î§ÎŸÎ£ 1: Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ Î»Î¹Î½Îº ÏƒÏ„Î¿ JS (Bootstrap Data); ---
        # Î‘Ï…Ï„ÏŒ Î»ÏÎ½ÎµÎ¹ Ï„Î¿ Ï€ÏÏŒÎ²Î»Î·Î¼Î± Î¼Îµ Ï„Î± Î»Î¹Î½Îº Ï€Î¿Ï… Î­ÏƒÏ„ÎµÎ¹Î»ÎµÏ‚
        bootstrap_link = extract_bootstrap_url(soup)
        
        if bootstrap_link and bootstrap_link.startswith('http'):
            # print(f"    -> Redirecting to embedded player: {bootstrap_link}")
            final_referer = bootstrap_link # O referer Î³Î¯Î½ÎµÏ„Î±Î¹ Î¿ embed player
            
            # Î Î·Î³Î±Î¯Î½Î¿Ï…Î¼Îµ ÎºÎ±Ï„ÎµÏ…Î¸ÎµÎ¯Î±Î½ ÏƒÏ„Î¿Î½ player (Ï€.Ï‡. upns.pro)
            sb.uc_open_with_reconnect(bootstrap_link, reconnect_time=3)
            
            # ÎšÎ»Î¹Îº ÏƒÏ„Î¿ Play (Î³Î¹Î± ÏƒÎ¹Î³Î¿Ï…ÏÎ¹Î¬)
            try: sb.click("video", timeout=1)
            except: pass
            try: sb.click(".jw-display-icon", timeout=1)
            except: pass
            
            sb.sleep(4) # Î ÎµÏÎ¹Î¼Î­Î½Î¿Ï…Î¼Îµ Î½Î± Ï†Î¿ÏÏ„ÏÏƒÎµÎ¹ Ï„Î¿ master file
            
            # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ ÏƒÏ„Î¿ source Ï„Î¿Ï… player
            player_source = sb.get_page_source()
            video_url, sub_url = extract_links_from_source(player_source)
            
            if video_url:
                return video_url, sub_url, final_referer

        # --- Î•Î›Î•Î“Î§ÎŸÎ£ 2: ÎšÎ±Î½Î¿Î½Î¹ÎºÏŒ ÏˆÎ¬Î¾Î¹Î¼Î¿ (Î‘Î½ Î´ÎµÎ½ Î²ÏÎ®ÎºÎ±Î¼Îµ bootstrap link) ---
        # Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ ÏƒÏ„Î·Î½ Ï„ÏÎ­Ï‡Î¿Ï…ÏƒÎ± ÏƒÎµÎ»Î¯Î´Î± (Î® Î±Î½ Ï„Î¿ bootstrap Î±Ï€Î­Ï„Ï…Ï‡Îµ)
        v, s = extract_links_from_source(source)
        if v: return v, s, final_referer

        # --- Î•Î›Î•Î“Î§ÎŸÎ£ 3: Iframe Deep Search ---
        iframes = sb.find_elements("iframe")
        if iframes:
            for i in range(len(iframes)):
                try:
                    current_iframes = sb.find_elements("iframe")
                    if i >= len(current_iframes): break
                    frame = current_iframes[i]
                    frame_src = frame.get_attribute("src")
                    if not frame_src or any(x in frame_src for x in ["google", "facebook", "twitter", "ads"]): continue
                    
                    sb.switch_to_frame(frame)
                    try: sb.click("video", timeout=1) 
                    except: pass
                    sb.sleep(3) 
                    
                    frame_source = sb.get_page_source()
                    v_frame, s_frame = extract_links_from_source(frame_source)
                    if v_frame:
                        video_url = v_frame
                        sub_url = s_frame
                        if frame_src.startswith("http"): final_referer = frame_src
                        sb.switch_to_default_content()
                        break
                    sb.switch_to_default_content()
                except: sb.switch_to_default_content()
                
    except Exception as e: 
        print(f"Error getting stream {watch_url}: {e}")
        
    return video_url, sub_url, final_referer

# --- SMART SAVE ---
def smart_save_m3u(new_streams):
    old_entries = []
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                lines = f.readlines()
            current_entry = {}
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    if current_entry: old_entries.append(current_entry)
                    title = line.split(",", 1)[1] if "," in line else "Unknown"
                    current_entry = {'title': title, 'raw_lines': [line]}
                elif line.startswith("#EXTVLCOPT") or line.startswith("http"):
                    if current_entry: current_entry['raw_lines'].append(line)
            if current_entry: old_entries.append(current_entry)
            print(f"ğŸ“‚ Loaded {len(old_entries)} existing movies.")
        except: pass

    new_titles = [s['title'] for s in new_streams]
    unique_old_entries = [entry for entry in old_entries if entry['title'] not in new_titles]
    print(f"â™»ï¸  Keeping {len(unique_old_entries)} older movies.")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for s in new_streams:
            clean_title = s['title'].replace(",", " -").replace("\n", " ")
            f.write(f"#EXTINF:-1 group-title=\"Movies\",{clean_title}\n")
            f.write(f"#EXTVLCOPT:http-referrer={s['referer']}/\n")
            f.write(f"#EXTVLCOPT:http-user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\n")
            if s['subtitle']: f.write(f"#EXTVLCOPT:sub-file={s['subtitle']}\n")
            f.write(f"{s['url']}\n")
        for entry in unique_old_entries:
            for line in entry['raw_lines']: f.write(f"{line}\n")
    print(f"âœ… Playlist updated! Total: {len(new_streams) + len(unique_old_entries)} movies.")

def main():
    all_streams = []
    with SB(uc=True, test=True, headless=False, xvfb=True) as sb:
        for list_url in START_URLS:
            print(f"Loading List: {list_url}")
            try:
                sb.uc_open_with_reconnect(list_url, reconnect_time=4)
                try: sb.uc_gui_click_captcha()
                except: pass
                sb.sleep(2)
                sb.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                sb.sleep(2)
                
                source = sb.get_page_source()
                soup = BeautifulSoup(source, 'html.parser')
                
                movie_links = []
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if '/titles/' in href and 'page=' not in href:
                        full_link = href if href.startswith('http') else BASE_URL + href
                        if full_link not in movie_links: movie_links.append(full_link)
                print(f"Found {len(movie_links)} movies on page.")
                
                for i, m_url in enumerate(movie_links):
                    print(f"Processing ({i+1}/{len(movie_links)}): {m_url}")
                    try:
                        sb.uc_open_with_reconnect(m_url, reconnect_time=2)
                        msource = sb.get_page_source()
                        msoup = BeautifulSoup(msource, 'html.parser')
                        title_tag = msoup.find('h1')
                        title = title_tag.text.strip() if title_tag else "Unknown"
                        
                        watch_url = None
                        label = "Stream"
                        # Î•Î´Ï Î²ÎµÎ»Ï„Î¹ÏÎ¸Î·ÎºÎµ Î· Î»Î¿Î³Î¹ÎºÎ®: Î‘Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Ï€Î¿Î»Î»Î¬ Î²Î¯Î½Ï„ÎµÎ¿, Ï€Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ 1Î¿ Ï€Î¿Ï… Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Trailer
                        for a in msoup.find_all('a', href=True):
                            if '/watch/' in a['href']:
                                temp_label = a.text.strip()
                                if "Trailer" in temp_label or "trailer" in temp_label.lower(): continue
                                if temp_label: label = temp_label
                                else: label = "Stream"
                                watch_url = a['href'] if a['href'].startswith('http') else BASE_URL + a['href']
                                break 
                        
                        if watch_url:
                            stream_link, sub_link, dynamic_referer = get_stream_and_sub(sb, watch_url)
                            if stream_link:
                                print(f"  + Found: {stream_link}")
                                stream_link = stream_link.split('"')[0].split("'")[0]
                                all_streams.append({
                                    'title': f"{title} [{label}]",
                                    'url': stream_link,
                                    'subtitle': sub_link,
                                    'referer': dynamic_referer
                                })
                            else: print(f"  - No link found in {watch_url}")
                        else: print("  - No watch button found")
                    except Exception as e: print(f"Error processing movie {m_url}: {e}")
            except Exception as e: print(f"Error on list {list_url}: {e}")

    if all_streams: smart_save_m3u(all_streams)
    else: print("âŒ No streams found.")

if __name__ == "__main__":
    main()
