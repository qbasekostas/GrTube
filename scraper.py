from seleniumbase import SB
from bs4 import BeautifulSoup
import re
import time
import os
import math
import json

BASE_URL = "https://greektube.pro"
START_URLS = [
    "https://greektube.pro/movies?order=created_at%3Adesc",
    "https://greektube.pro/movies?order=created_at%3Adesc&page=2"
]
OUTPUT_FILE = "GrTube.m3u"
BATCH_SIZE = 5

def close_popups(sb, main_window):
    try:
        if len(sb.driver.window_handles) > 1:
            for handle in sb.driver.window_handles:
                if handle != main_window:
                    sb.driver.switch_to.window(handle)
                    sb.driver.close()
            sb.driver.switch_to.window(main_window)
    except: pass

def get_network_video(sb):
    """Network Sniffer (DevTools)"""
    try:
        logs = sb.execute_script("""
            return window.performance.getEntriesByType("resource")
                .map(r => r.name)
                .filter(n => n.match(/\.(mp4|m3u8|txt)|master/));
        """)
        for url in reversed(logs):
            if any(ext in url for ext in ['.mp4', '.m3u8', '.txt']) and not any(bad in url for bad in ['google', 'facebook', 'analytics', 'svg', 'jpg']):
                return url
    except: pass
    return None

def extract_bootstrap_link(soup):
    """Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï„Î¿ link Î±Ï€ÏŒ Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± (JSON)"""
    try:
        scripts = soup.find_all('script')
        for s in scripts:
            if s.string and 'window.bootstrapData' in s.string:
                # Regex Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ src Î³ÏÎ®Î³Î¿ÏÎ± ÎºÎ±Î¹ Î²ÏÏÎ¼Î¹ÎºÎ±
                match = re.search(r'"src"\s*:\s*"([^"]+)"', s.string)
                if not match: match = re.search(r"'src'\s*:\s*'([^']+)'", s.string)
                
                if match:
                    url = match.group(1).replace(r'\/', '/')
                    if "http" in url and ("upns" in url or "embed" in url or "greektube" in url):
                        return url
    except: pass
    return None

def get_stream_with_devtools(sb, watch_url):
    final_referer = watch_url
    video_url = None
    sub_url = None

    try:
        if sb.get_current_url() != watch_url:
            sb.uc_open_with_reconnect(watch_url, reconnect_time=3)
        
        main_win = sb.driver.current_window_handle
        time.sleep(2)

        # 1. Î¨Î¬Ï‡Î½Î¿Ï…Î¼Îµ Ï„Î¿ Link ÏƒÏ„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± (JSON/Regex)
        soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
        target_url = extract_bootstrap_link(soup)

        # 2. Î‘Î½ Î´ÎµÎ½ Ï„Î¿ Î²ÏÎ®ÎºÎ±Î¼Îµ ÏƒÏ„Î¿ JSON, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ Î³Î¹Î± Iframe (Î§Î©Î¡Î™Î£ ÎÎ‘ ÎšÎ¡Î‘Î£Î‘Î¡ÎŸÎ¥ÎœÎ•)
        if not target_url:
            try:
                # Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ find_elements Ï€Î¿Ï… Î´ÎµÎ½ Ï€ÎµÏ„Î¬ÎµÎ¹ error Î±Î½ ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î¿
                iframes = sb.driver.find_elements("css selector", "iframe")
                for frame in iframes:
                    src = frame.get_attribute("src")
                    if src and "google" not in src:
                        target_url = src
                        break
            except: pass

        # 3. Î Î¬Î¼Îµ ÏƒÏ„Î¿Î½ Player
        if target_url:
            if not target_url.startswith("http"): target_url = BASE_URL + target_url
            # print(f"    -> Redirect: {target_url}")
            if target_url != watch_url:
                sb.uc_open_with_reconnect(target_url, reconnect_time=3)
                final_referer = target_url
                main_win = sb.driver.current_window_handle

        # 4. ÎšÎ›Î™Îš & SNIFF (Î— Î²Î±ÏƒÎ¹ÎºÎ® Î´Î¿Ï…Î»ÎµÎ¹Î¬)
        time.sleep(1)
        close_popups(sb, main_win)
        
        # Î¤Ï…Ï†Î»Î¬ ÎºÎ»Î¹Îº Î³Î¹Î± Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÎ¹ Ï„Î¿ Î²Î¯Î½Ï„ÎµÎ¿
        click_targets = ["video", "#player", ".jw-display-icon", ".play-button", "body", "div[id*='player']"]
        for target in click_targets:
            try: 
                sb.click(target, timeout=0.5)
                close_popups(sb, main_win) # ÎšÎ»ÎµÎ¯ÏƒÎ¹Î¼Î¿ Î±Î½ Ï€ÎµÏ„Î¬Ï‡Ï„Î·ÎºÎµ Î´Î¹Î±Ï†Î®Î¼Î¹ÏƒÎ·
            except: pass

        time.sleep(4) # Î‘Î½Î±Î¼Î¿Î½Î® Î³Î¹Î± Network
        
        # 5. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Network Logs
        video_url = get_network_video(sb)

        # Fallback: Î‘Î½ Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î¿ network, ÏˆÎ¬Ï‡Î½Î¿Ï…Î¼Îµ ÏƒÏ„Î¿ source Î¼Î®Ï€Ï‰Ï‚ Î³ÏÎ¬Ï†Ï„Î·ÎºÎµ ÎµÎºÎµÎ¯
        if not video_url:
            src = sb.get_page_source().replace(r'\/', '/')
            match = re.search(r'(https?://[^"\'<>\s]+\.(?:mp4|m3u8|txt)(?:[^"\'<>\s]*)?)', src)
            if match and "google" not in match.group(1):
                video_url = match.group(1)

        # Subs
        sub_match = re.search(r'(https?://[^"\'<>\s]+\.(?:vtt|srt)(?:[^"\'<>\s]*)?)', sb.get_page_source().replace(r'\/', '/'))
        if sub_match: sub_url = sub_match.group(1)

    except Exception as e:
        print(f"    Gen Error: {e}")
        try: sb.driver.switch_to.window(sb.driver.window_handles[0])
        except: pass

    return video_url, sub_url, final_referer

def smart_save_m3u(new_streams):
    old_entries = []
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                lines = f.readlines()
            current_entry = {}
            for line in lines:
                line = line.strip()
                if line.startswith("#EXTINF"):
                    if current_entry: old_entries.append(current_entry)
                    title = line.split(",", 1)[1] if "," in line else "Unknown"
                    current_entry = {'title': title, 'raw_lines': [line]}
                elif line.startswith("#EXTVLCOPT") or line.startswith("http"):
                    if current_entry: current_entry['raw_lines'].append(line)
            if current_entry: old_entries.append(current_entry)
            print(f"ğŸ“‚ Loaded {len(old_entries)} existing movies.")
        except: pass

    new_titles = [s['title'] for s in new_streams]
    unique_old_entries = [entry for entry in old_entries if entry['title'] not in new_titles]

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for s in new_streams:
            clean_title = s['title'].replace(",", " -").replace("\n", " ")
            f.write(f"#EXTINF:-1 group-title=\"Movies\",{clean_title}\n")
            f.write(f"#EXTVLCOPT:http-referrer={s['referer']}/\n")
            f.write(f"#EXTVLCOPT:http-user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\n")
            if s['subtitle']: f.write(f"#EXTVLCOPT:sub-file={s['subtitle']}\n")
            f.write(f"{s['url']}\n")
        for entry in unique_old_entries:
            for line in entry['raw_lines']: f.write(f"{line}\n")
    print(f"âœ… Playlist updated! Total: {len(new_streams) + len(unique_old_entries)} movies.")

def get_all_movie_urls():
    movie_links = []
    print("ğŸ”µ Phase 1: Collecting URLs...")
    with SB(uc=True, test=True, headless=False, xvfb=True, block_images=False) as sb:
        for list_url in START_URLS:
            try:
                sb.uc_open_with_reconnect(list_url, reconnect_time=5)
                if "Just a moment" in sb.get_title():
                    sb.uc_gui_click_captcha(); sb.sleep(3)
                
                sb.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                sb.sleep(2)
                
                # Î‘Ï€Î»ÏŒ ÎºÎ±Î¹ Î³ÏÎ®Î³Î¿ÏÎ¿ BeautifulSoup Î³Î¹Î± Ï„Î± links
                soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if '/titles/' in href and 'page=' not in href:
                        full_link = href if href.startswith('http') else BASE_URL + href
                        if full_link not in movie_links: movie_links.append(full_link)
            except: pass
    print(f"ğŸŸ¢ Found {len(movie_links)} movies.")
    return movie_links

def process_batch(links):
    batch_streams = []
    with SB(uc=True, test=True, headless=False, xvfb=True, block_images=False) as sb:
        for url in links:
            print(f"   Processing: {url}")
            try:
                sb.uc_open_with_reconnect(url, reconnect_time=4)
                if "Just a moment" in sb.get_title():
                    try: sb.uc_gui_click_captcha(); sb.sleep(3)
                    except: pass
                
                # Check popups
                handle_window = sb.driver.current_window_handle
                close_popups(sb, handle_window)

                soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
                title_tag = soup.find('h1')
                if not title_tag: continue
                title = title_tag.text.strip()
                
                watch_url = None
                label = "Stream"
                
                # 1. Search Buttons
                for a in soup.find_all('a', href=True):
                    if '/watch/' in a['href']:
                        txt = a.text.strip().lower()
                        if any(x in txt for x in ["trailer", "teaser", "clip"]): continue
                        watch_url = a['href'] if a['href'].startswith('http') else BASE_URL + a['href']
                        break 
                
                # 2. Header Button
                if not watch_url:
                    for a in soup.find_all('a', href=True):
                        if '/watch/' in a['href'] and ('Î´ÎµÎ¯Ï„Îµ' in a.text.lower() or 'play' in a.text.lower()):
                            watch_url = a['href'] if a['href'].startswith('http') else BASE_URL + a['href']
                            break

                target = watch_url if watch_url else url 
                v, s, r = get_stream_with_devtools(sb, target)
                
                if v:
                    v = v.split('"')[0].split("'")[0]
                    print(f"     + Found: {v}")
                    batch_streams.append({'title': title, 'url': v, 'subtitle': s, 'referer': r})
                else:
                    print("     - No stream found.")

            except Exception as e: print(f"    Skipped: {e}")
            
    return batch_streams

def main():
    all_links = get_all_movie_urls()
    if not all_links: 
        # Create empty file to avoid git error
        with open(OUTPUT_FILE, "w") as f: f.write("")
        return
    
    total_streams = []
    num_batches = math.ceil(len(all_links) / BATCH_SIZE)
    
    for i in range(num_batches):
        print(f"ğŸŸ  Batch {i+1}/{num_batches}...")
        batch = all_links[i*BATCH_SIZE : (i+1)*BATCH_SIZE]
        try:
            res = process_batch(batch)
            total_streams.extend(res)
        except: pass
        time.sleep(2)
        
    if total_streams: smart_save_m3u(total_streams)
    else: 
        print("âŒ No streams.")
        if not os.path.exists(OUTPUT_FILE):
             with open(OUTPUT_FILE, "w") as f: f.write("")

if __name__ == "__main__":
    main()
